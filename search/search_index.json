{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>pet_fac_rec</code>","text":""},{"location":"#overview","title":"Overview","text":"<p>Welcome to the documentation for <code>pet_fac_rec</code>, a machine learning project for pet facial recognition. This documentation provides details on the following: - Project architecture - Data preprocessing - Model design and performance</p>"},{"location":"#project-architecture","title":"Project Architecture","text":"<p>The following diagram illustrates the architecture of the project:</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Data Handling: Manages the datasets for training and validation.</li> <li>Model Training: Supports multiple deep learning models like EfficientNet, ResNet, and VGG. EfficientNet is the main model that is used.</li> <li>Visualization: Visualizes training and testing losses and accuracies.</li> </ul>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li>Data: Learn more about the dataset and preprocessing steps.</li> <li>Model: Explore the model details and training process.</li> <li>Visualization: Visualize the training progress and results.</li> </ul>"},{"location":"data/","title":"The Data","text":""},{"location":"data/#pet_fac_rec.data.MyDataset","title":"pet_fac_rec.data.MyDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Custom dataset for processing images and labels from a CSV file.</p> Source code in <code>pet_fac_rec/data.py</code> <pre><code>class MyDataset(Dataset):\n    \"\"\"Custom dataset for processing images and labels from a CSV file.\"\"\"\n\n    def __init__(self, csv_file: Path, split: str, transform: Optional[transforms.Compose] = None) -&gt; None:\n        \"\"\"\n        Initialize the dataset from a CSV file.\n\n        Args:\n            csv_file (Path): Path to the CSV file containing file paths and labels.\n            transform (Optional[transforms.Compose]): Transformations to apply to the images.\n        \"\"\"\n        if not csv_file.exists():\n            raise FileNotFoundError(f\"CSV file not found: {csv_file}\")\n        self.data = pd.read_csv(csv_file)\n        self.data = self.data[self.data[\"split\"] == split]\n        self.labels = self.data[\"label\"].unique()\n        self.name = split\n        self.images = list(self.data[\"file_path\"])\n        self.target = torch.tensor(self.data[\"label\"].values, dtype=torch.long)\n        self.num_classes = len(self.labels)\n        self.transform = transform or get_transforms()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int) -&gt; Tuple[Image.Image, int]:\n        \"\"\"Return a specific sample from the dataset.\n\n        Args:\n            index (int): Index of the sample to retrieve.\n\n        Returns:\n            Tuple[Image.Image, int]: Transformed image and its label.\n        \"\"\"\n        row = self.data.iloc[index]\n        image_path = Path(row[\"file_path\"])\n        label = int(row[\"label\"])\n\n        # Load and transform the image\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n</code></pre>"},{"location":"data/#pet_fac_rec.data.MyDataset-functions","title":"Functions","text":""},{"location":"data/#pet_fac_rec.data.MyDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; Tuple[Image.Image, int]\n</code></pre> <p>Return a specific sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, int]</code> <p>Tuple[Image.Image, int]: Transformed image and its label.</p> Source code in <code>pet_fac_rec/data.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Tuple[Image.Image, int]:\n    \"\"\"Return a specific sample from the dataset.\n\n    Args:\n        index (int): Index of the sample to retrieve.\n\n    Returns:\n        Tuple[Image.Image, int]: Transformed image and its label.\n    \"\"\"\n    row = self.data.iloc[index]\n    image_path = Path(row[\"file_path\"])\n    label = int(row[\"label\"])\n\n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    if self.transform:\n        image = self.transform(image)\n\n    return image, label\n</code></pre>"},{"location":"data/#pet_fac_rec.data.MyDataset.__init__","title":"__init__","text":"<pre><code>__init__(csv_file: Path, split: str, transform: Optional[transforms.Compose] = None) -&gt; None\n</code></pre> <p>Initialize the dataset from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>Path</code> <p>Path to the CSV file containing file paths and labels.</p> required <code>transform</code> <code>Optional[Compose]</code> <p>Transformations to apply to the images.</p> <code>None</code> Source code in <code>pet_fac_rec/data.py</code> <pre><code>def __init__(self, csv_file: Path, split: str, transform: Optional[transforms.Compose] = None) -&gt; None:\n    \"\"\"\n    Initialize the dataset from a CSV file.\n\n    Args:\n        csv_file (Path): Path to the CSV file containing file paths and labels.\n        transform (Optional[transforms.Compose]): Transformations to apply to the images.\n    \"\"\"\n    if not csv_file.exists():\n        raise FileNotFoundError(f\"CSV file not found: {csv_file}\")\n    self.data = pd.read_csv(csv_file)\n    self.data = self.data[self.data[\"split\"] == split]\n    self.labels = self.data[\"label\"].unique()\n    self.name = split\n    self.images = list(self.data[\"file_path\"])\n    self.target = torch.tensor(self.data[\"label\"].values, dtype=torch.long)\n    self.num_classes = len(self.labels)\n    self.transform = transform or get_transforms()\n</code></pre>"},{"location":"data/#pet_fac_rec.data.MyDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>pet_fac_rec/data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"data/#pet_fac_rec.data.preprocess","title":"pet_fac_rec.data.preprocess","text":"<pre><code>preprocess(output_folder: Path) -&gt; None\n</code></pre> <p>Preprocess the raw data and save it to the output folder. Downloads the dataset if not already present and generates a CSV file.</p> Source code in <code>pet_fac_rec/data.py</code> <pre><code>def preprocess(output_folder: Path) -&gt; None:\n    \"\"\"\n    Preprocess the raw data and save it to the output folder.\n    Downloads the dataset if not already present and generates a CSV file.\n    \"\"\"\n    log.info(\"Preprocessing data...\")\n\n    # Ensure the output folder exists\n    output_folder.mkdir(parents=True, exist_ok=True)\n\n    # Check if the dataset has already been moved\n    if any((output_folder / split).exists() for split in [\"train\", \"valid\", \"test\"]):\n        log.info(\"Dataset already exists. Skipping download.\")\n        return\n\n    log.info(\"Downloading dataset...\")\n    download_path = Path(kh.dataset_download(\"anshtanwar/pets-facial-expression-dataset\"))\n    log.info(f\"Dataset downloaded to: {download_path}\")\n\n    if not download_path.exists():\n        log.info(\"Error: Download path does not exist. Dataset download failed.\")\n        return\n\n    # Check the contents of the downloaded path\n    log.info(f\"Contents of the downloaded dataset: {[item.name for item in download_path.iterdir()]}\")\n\n    master_folder_path = download_path / \"Master Folder\"\n\n    if master_folder_path.exists():\n        log.info(f\"Contents of 'Master Folder': {[item.name for item in master_folder_path.iterdir()]}\")\n\n        # Move the subdirectories (train, valid, test) from \"Master Folder\" to the output folder\n        for subfolder in [\"train\", \"valid\", \"test\"]:\n            source = master_folder_path / subfolder\n            destination = output_folder / subfolder\n            if source.exists():\n                shutil.move(str(source), str(destination))\n                log.info(f\"Moved '{subfolder}' to {output_folder}\")\n            else:\n                log.info(f\"Warning: '{subfolder}' not found in 'Master Folder'.\")\n    else:\n        log.info(\"Error: 'Master Folder' not found in the downloaded dataset.\")\n        return\n\n    # Walk through each subdirectory and file to create a data list\n    data = []\n\n    for split in [\"train\", \"valid\", \"test\"]:\n        split_path = output_folder / split\n        if not split_path.exists():\n            log.info(f\"Warning: Split folder '{split}' not found in {output_folder}\")\n            continue\n        for label_dir in split_path.iterdir():\n            if label_dir.is_dir():\n                for file in label_dir.iterdir():\n                    if file.is_file():\n                        # Map the label and store file path and label\n                        data.append(\n                            {\n                                \"split\": split,\n                                \"label\": label_mapping.get(label_dir.name.lower(), -1),\n                                \"file_path\": str(file),\n                            }\n                        )\n\n    # Convert the list to a DataFrame and save it as a CSV\n    data_df = pd.DataFrame(data)\n    csv_path = output_folder / \"data.csv\"\n    data_df.to_csv(csv_path, index=False)\n    log.info(f\"Data saved to CSV at: {csv_path}\")\n</code></pre>"},{"location":"model/","title":"The model","text":""},{"location":"model/#pet_fac_rec.model.MyEfficientNetModel","title":"pet_fac_rec.model.MyEfficientNetModel","text":"<p>               Bases: <code>Module</code></p> <p>A model leveraging EfficientNet as the base feature extractor.</p> Source code in <code>pet_fac_rec/model.py</code> <pre><code>class MyEfficientNetModel(nn.Module):\n    \"\"\"\n    A model leveraging EfficientNet as the base feature extractor.\n    \"\"\"\n\n    def __init__(self, num_classes: int, pretrained: bool = True, dropout_rate: float = 0.5) -&gt; None:\n        super().__init__()\n        self.base_model = efficientnet_b5(pretrained=pretrained)\n        num_features = self.base_model.classifier[1].in_features\n        self.base_model.classifier = nn.Sequential(\n            # 1st layer\n            nn.Linear(num_features, num_features // 2),\n            nn.BatchNorm1d(num_features // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            # 2nd layer\n            nn.Linear(num_features // 2, num_features // 4),\n            nn.BatchNorm1d(num_features // 4),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            # Output layer\n            nn.Linear(num_features // 4, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.base_model(x)\n</code></pre>"},{"location":"model/#pet_fac_rec.train.train","title":"pet_fac_rec.train.train","text":"<pre><code>train(model_name: str = typer.Option('efficientnet', help=\"Model type to use ('efficientnet', 'resnet50', 'vgg16')\"), overrides: Optional[List[str]] = typer.Argument(None)) -&gt; None\n</code></pre> <p>Train a model with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The type of model to use ('efficientnet', 'resnet50', 'vgg16').</p> <code>Option('efficientnet', help=\"Model type to use ('efficientnet', 'resnet50', 'vgg16')\")</code> <code>overrides</code> <code>Optional[List[str]]</code> <p>A list of configuration overrides.</p> <code>Argument(None)</code> Source code in <code>pet_fac_rec/train.py</code> <pre><code>@app.command()\ndef train(\n    model_name: str = typer.Option(\"efficientnet\", help=\"Model type to use ('efficientnet', 'resnet50', 'vgg16')\"),\n    overrides: Optional[List[str]] = typer.Argument(None),\n) -&gt; None:\n    \"\"\"\n    Train a model with the specified configuration.\n\n    Args:\n        model_name (str): The type of model to use ('efficientnet', 'resnet50', 'vgg16').\n        overrides (Optional[List[str]]): A list of configuration overrides.\n    \"\"\"\n    cfg = my_compose(overrides)\n    log.info(f\"Configuration: {OmegaConf.to_yaml(cfg)}\")\n    hparams = cfg.experiment\n\n    # Set the seed for reproducibility\n    set_seed(hparams.seed)\n\n    # Determine the device to use for training\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    )\n    log.info(f\"Running on dev: {device}\")  # Remove later\n\n    run = wandb.init(\n        project=WANDB_PROJECT,\n        entity=WANDB_ENTITY,\n        job_type=\"train\",\n        name=f\"exp_{current_time}\",\n        config={\n            \"lr\": hparams.lr,\n            \"batch_size\": hparams.batch_size,\n            \"epochs\": hparams.epochs,\n            \"dropout\": hparams.dropout,\n        },\n    )\n\n    # Load the dataset\n    transform = get_transforms()\n    train_dataset = MyDataset(csv_file=Path(hparams.data_csv), split=\"train\", transform=transform)\n    valid_dataset = MyDataset(csv_file=Path(hparams.data_csv), split=\"valid\", transform=transform)\n    train_dataloader = DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=True)\n    val_dataloader = DataLoader(valid_dataset, batch_size=hparams.batch_size, shuffle=True)\n\n    # Initialize the model\n    num_classes = train_dataset.num_classes\n    model = get_model(model_name, num_classes, hparams.dropout).to(device)\n\n    # Define loss function and optimizer\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=hparams.lr)\n\n    train_losses = []\n    train_accuracies = []\n\n    val_losses = []\n    val_accuracies = []\n\n    # Training loop\n    log.info(f\"Start training {model_name}...\")\n    epoch_bar = tqdm(range(hparams.epochs))\n    for epoch in epoch_bar:\n        with torch.profiler.profile(\n            schedule=torch.profiler.schedule(wait=2, warmup=2, active=6, repeat=1),\n            on_trace_ready=torch.profiler.tensorboard_trace_handler(\"profiler\"),\n            with_stack=True,\n        ) as profiler:\n            model.train()\n            train_loss = 0.0\n            total_correct = 0\n            total_samples = 0\n\n            for img, target in iter(train_dataloader):\n                img, target = img.to(device), target.to(device)\n\n                # Forward pass\n                y_pred = model(img)\n                with record_function(\"model_loss\"):\n                    loss = criterion(y_pred, target)\n\n                # Backward pass and optimization\n                optimizer.zero_grad()\n                with record_function(\"backward\"):\n                    loss.backward()\n                    optimizer.step()\n                profiler.step()\n\n                train_loss += loss.item() * img.size(0)\n                _, preds = torch.max(y_pred, 1)\n                total_correct += (preds == target).sum().item()\n                total_samples += target.size(0)\n\n            train_losses.append(train_loss / total_samples)\n            train_accuracies.append(total_correct / total_samples)\n\n            val_loss = 0.0\n            total_correct = 0\n            total_samples = 0\n\n            model.eval()\n\n            with torch.no_grad():\n                for img, target in iter(val_dataloader):\n                    img, target = img.to(device), target.to(device)\n\n                    outputs = model(img)\n                    loss = criterion(outputs, target)\n\n                    val_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    total_correct += (preds == target).sum().item()\n                    total_samples += target.size(0)\n\n            val_losses.append(val_loss / total_samples)\n            val_accuracies.append(total_correct / total_samples)\n            status = (\n                f\"Epoch {epoch + 1}/{hparams.epochs} | \"\n                f\"Train Loss: {train_losses[-1]:.5f} | \"\n                f\"Train Acc: {train_accuracies[-1]:.5f} | \"\n                f\"Val Loss: {val_losses[-1]:.5f} | \"\n                f\"Val Acc: {val_accuracies[-1]:.5f}\"\n            )\n            epoch_bar.set_description(status)\n            log.info(status)\n            wandb.log(\n                {\n                    \"train_loss\": train_losses[-1],\n                    \"train_accuracy\": train_accuracies[-1],\n                    \"valid_loss\": val_losses[-1],\n                    \"valid_accuracy\": val_accuracies[-1],\n                }\n            )\n\n    log.info(\"Training complete\")\n\n    # Save the model\n    model_save_path = f\"models/{model_name}.pth\"\n    torch.save(model.state_dict(), model_save_path)\n    log.info(f\"Model saved to {model_save_path}\")\n\n    save_and_log_artifact(wandb, run, model_save_path, train_losses, train_accuracies, val_losses, val_accuracies)\n    save_onnx_file(model_name, model, device)\n\n    # Plot training statistics\n    plot_training_statistics(train_losses, train_accuracies, val_losses, val_accuracies)\n\n    wandb.finish()\n</code></pre>"},{"location":"model/#pet_fac_rec.evaluate.evaluate","title":"pet_fac_rec.evaluate.evaluate","text":"<pre><code>evaluate(model_name: str = typer.Option('efficientnet', help=\"Model type to use ('efficientnet', 'resnet50', 'vgg16')\"), model_checkpoint: str = typer.Option(..., help='Path to the model checkpoint file (e.g., efficientnet.pth)'), data_csv: Path = Path('data/data.csv')) -&gt; None\n</code></pre> <p>Evaluate a trained model.</p> Source code in <code>pet_fac_rec/evaluate.py</code> <pre><code>@app.command()\ndef evaluate(\n    model_name: str = typer.Option(\"efficientnet\", help=\"Model type to use ('efficientnet', 'resnet50', 'vgg16')\"),\n    model_checkpoint: str = typer.Option(..., help=\"Path to the model checkpoint file (e.g., efficientnet.pth)\"),\n    data_csv: Path = Path(\"data/data.csv\"),\n) -&gt; None:\n    \"\"\"\n    Evaluate a trained model.\n    \"\"\"\n    load_dotenv()\n    wandb.init(\n        project=\"pet_fac_rec\",\n        entity=\"luyentrungkien00-danmarks-tekniske-universitet-dtu\",\n        job_type=\"evaluate\",\n        name=f\"eval_exp_{model_name}_{CURR_TIME}\",\n    )\n\n    log.info(\"Evaluating...\")\n    log.info(f\"Model: {model_name}\")\n    log.info(f\"Checkpoint: {model_checkpoint}\")\n\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    )\n\n    # Load the dataset\n    test_set = MyDataset(csv_file=data_csv, split=\"test\", transform=get_transforms())\n    test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False)\n\n    # Initialize the model\n    num_classes = test_set.num_classes\n    model = get_model(model_name, num_classes).to(device)\n\n    # Load model checkpoint\n    model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n\n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n    for img, target in test_dataloader:\n        img, target = img.to(device), target.to(device)\n        y_pred = model(img)\n        _, preds = torch.max(y_pred, 1)\n        correct += (preds == target).sum().item()\n        total += target.size(0)\n\n    test_acc = correct / total\n\n    log.info(f\"Test accuracy: {correct / total:.5f}\")\n    wandb.log({\"test_accuracy\": test_acc})\n    wandb.finish()\n</code></pre>"},{"location":"model/#other-models","title":"Other models","text":""},{"location":"model/#pet_fac_rec.model.MyResNet50Model","title":"pet_fac_rec.model.MyResNet50Model","text":"<p>               Bases: <code>Module</code></p> <p>A model leveraging ResNet50 as the base feature extractor.</p> Source code in <code>pet_fac_rec/model.py</code> <pre><code>class MyResNet50Model(nn.Module):\n    \"\"\"\n    A model leveraging ResNet50 as the base feature extractor.\n    \"\"\"\n\n    def __init__(self, num_classes: int, pretrained: bool = True) -&gt; None:\n        super().__init__()\n        self.base_model = resnet50(pretrained=pretrained)\n        num_features = self.base_model.fc.in_features\n        self.base_model.fc = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.base_model(x)\n</code></pre>"},{"location":"model/#pet_fac_rec.model.MyVGG16Model","title":"pet_fac_rec.model.MyVGG16Model","text":"<p>               Bases: <code>Module</code></p> <p>A model leveraging VGG16 as the base feature extractor.</p> Source code in <code>pet_fac_rec/model.py</code> <pre><code>class MyVGG16Model(nn.Module):\n    \"\"\"\n    A model leveraging VGG16 as the base feature extractor.\n    \"\"\"\n\n    def __init__(self, num_classes: int, pretrained: bool = True) -&gt; None:\n        super().__init__()\n        self.base_model = vgg16(pretrained=pretrained)\n        num_features = self.base_model.classifier[6].in_features\n        self.base_model.classifier[6] = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.base_model(x)\n</code></pre>"},{"location":"visualization/","title":"Visualization","text":"<p>The following diagram illustrates the training statistics:</p> <p></p>"},{"location":"visualization/#pet_fac_rec.visualize.plot_training_statistics","title":"pet_fac_rec.visualize.plot_training_statistics","text":"<pre><code>plot_training_statistics(train_losses, train_accuracies, val_losses, val_accuracies, output_path='reports/figures/training_statistics.png')\n</code></pre> <p>Plot training and validation statistics.</p> <p>Parameters:</p> Name Type Description Default <code>train_losses</code> <code>list</code> <p>Training losses over epochs.</p> required <code>train_accuracies</code> <code>list</code> <p>Training accuracies over epochs.</p> required <code>val_losses</code> <code>list</code> <p>Validation losses over epochs.</p> required <code>val_accuracies</code> <code>list</code> <p>Validation accuracies over epochs.</p> required <code>output_path</code> <code>str</code> <p>Path to save the plot image.</p> <code>'reports/figures/training_statistics.png'</code> Source code in <code>pet_fac_rec/visualize.py</code> <pre><code>def plot_training_statistics(\n    train_losses, train_accuracies, val_losses, val_accuracies, output_path=\"reports/figures/training_statistics.png\"\n):\n    \"\"\"\n    Plot training and validation statistics.\n\n    Args:\n        train_losses (list): Training losses over epochs.\n        train_accuracies (list): Training accuracies over epochs.\n        val_losses (list): Validation losses over epochs.\n        val_accuracies (list): Validation accuracies over epochs.\n        output_path (str): Path to save the plot image.\n    \"\"\"\n    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Plot Training Loss\n    axs[0][0].plot(train_losses, label=\"Train Loss\")\n    axs[0][0].set_title(\"Train Loss\")\n    axs[0][0].set_xlabel(\"Epochs\")\n    axs[0][0].set_ylabel(\"Loss\")\n    axs[0][0].legend()\n\n    # Plot Training Accuracy\n    axs[0][1].plot(train_accuracies, label=\"Train Accuracy\")\n    axs[0][1].set_title(\"Train Accuracy\")\n    axs[0][1].set_xlabel(\"Epochs\")\n    axs[0][1].set_ylabel(\"Accuracy\")\n    axs[0][1].legend()\n\n    # Plot Validation Loss\n    axs[1][0].plot(val_losses, label=\"Validation Loss\")\n    axs[1][0].set_title(\"Validation Loss\")\n    axs[1][0].set_xlabel(\"Epochs\")\n    axs[1][0].set_ylabel(\"Loss\")\n    axs[1][0].legend()\n\n    # Plot Validation Accuracy\n    axs[1][1].plot(val_accuracies, label=\"Validation Accuracy\")\n    axs[1][1].set_title(\"Validation Accuracy\")\n    axs[1][1].set_xlabel(\"Epochs\")\n    axs[1][1].set_ylabel(\"Accuracy\")\n    axs[1][1].legend()\n\n    # Adjust layout and save the figure\n    plt.tight_layout()\n    fig.savefig(output_path)\n    log.info(f\"Training statistics saved to {output_path}\")\n</code></pre>"}]}